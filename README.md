# Источники данных на PySpark

## Цель работы
Целью данной работы является получение навыков выгрузки исходных данных и отправки результатов модели с использованием различных источников данных согласно варианту задания.

## Выполнение работы
1. Была написан docker-compose файл для запуска MsSQL базы данных.
2. Была произведена модификация Dockerfile и docker-compose файлов приложения для взаимодействия с MsSQL базой данных. 
3. Был написан скрипт `src/load_data_to_db.py` для заполнения базы данных данными из csv-файлов.
4. Был модифицирован скрипт `src/kmeans.py` для взаимодействия с MsSQL базой данных. Обеспечена выгрузка при каждом запуске и загрузка данных после каждого запуска обучения модели.
5. Для взаимодействия с базой данных используется библиотека `pyodbc` и встроенные в Spark возможности работы с JDBC.

## Запуск среды
Для запуска среды нужно выполнить следующие команды из корня репозитория:
1. `docker-compose build`
2. `docker-compose up -d`
3. `docker exec -i -t [container id] bash`

Далее, для запуска Jupyter Notebook (ноутбук будет доступен на порту 8888):
1. `jupyter notebook --ip 0.0.0.0 --no-browser --allow-root`

Для запуска скриптов нужно перейти в папку src Docker контейнера и выполнить команду:
1. `python [script name]`

Перед запуском `src/kmeans.py` нужно запустить `src/load_data_to_db.py` для заполнения базы данных.

Во время запущенного Jupyter Notebook вы можете перейти по адресу `localhost:4444` для доступа к Spark веб-интерфейсу.
Также доступ можно получить и при запуске `src`-кода, для этого нужно добавить вызов функции `keep_spark_web_ui_alive` из `utils.spark` перед закрытием `SparkSession`.

## Результаты работы
1. Был добавлен сервис MsSQL базы данных.
2. Был связан функционал приложения с MsSQL базой данных.
